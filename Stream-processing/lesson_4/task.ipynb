{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78a2bd5c-bf93-4198-9ee2-4e512d30ff6a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Топик успешно создан: lesson3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StringType, IntegerType\n",
    "from pyspark.sql.functions import from_json, to_json, col, struct\n",
    "import time\n",
    "import signal\n",
    "from typing import Union\n",
    "\n",
    "from tools_kafka import Kafka\n",
    "from tools_pyspark_hdfs import Spark_HDFS as HDFS\n",
    "from tools_pyspark import stop_all_streams, sink, console_output\n",
    "\n",
    "def read_stream(spark, topic_name, schema, kf):\n",
    "    \"\"\"Возвращает стрим топика кафки\"\"\"\n",
    "    return spark.readStream \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", kf.SERVERS) \\\n",
    "        .option(\"subscribe\", topic_name) \\\n",
    "        .option(\"startingOffsets\", \"earliest\") \\\n",
    "        .option(\"maxOffsetsPerTrigger\", \"5\") \\\n",
    "        .load() \\\n",
    "        .select(F.from_json(F.col(\"value\").cast(\"String\"), schema).alias(\"value\"), \"offset\") \\\n",
    "        .select(\"value.*\", \"offset\")\n",
    "\n",
    "spark = SparkSession.builder.appName(\"my_spark\").getOrCreate()\n",
    "\n",
    "kf = Kafka()\n",
    "hdfs = HDFS(spark)\n",
    "\n",
    "topic_name = 'lesson3'\n",
    "sink_path = f'tmp_{topic_name}'  # в случае с kafka это название топика, где хранится информация\n",
    "checkpoint = f'sink_{sink_path}'\n",
    "\n",
    "schema = StructType() \\\n",
    "    .add(\"column_1\", StringType()) \\\n",
    "    .add(\"column_2\", IntegerType())\n",
    "\n",
    "kf.add('lesson3') # создаём топик кафки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63570425-aa3c-4fd4-a70d-de32e3447808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"column_1\":\"a\",\"column_2\":2}\n",
      "{\"column_1\":\"b\",\"column_2\":4}\n",
      "{\"column_1\":\"c\",\"column_2\":8}\n",
      "Stopping stream: <pyspark.sql.streaming.StreamingQuery object at 0x7d604ef353c0>\n"
     ]
    }
   ],
   "source": [
    "# Читаем файл и переносим его в топик кафки\n",
    "stream = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .schema(schema) \\\n",
    "    .csv(\"/data/\") \\\n",
    "    .selectExpr(\"CAST(null AS STRING) as key\", \n",
    "                \"CAST(to_json(struct(*)) AS STRING) as value\") \\\n",
    "    .writeStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kf.SERVERS) \\\n",
    "    .option(\"topic\", topic_name) \\\n",
    "    .option(\"checkpointLocation\", \"checkpoints/stream_read_write\") \\\n",
    "    .start()\n",
    "\n",
    "# выводим содержимое топика\n",
    "kf.get(topic_name) \n",
    "# остановим потоки\n",
    "stop_all_streams(spark) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e514ce1d-aa5a-4afc-b059-220a40b380ca",
   "metadata": {},
   "source": [
    "### Попробуем сохранить данные в sink тремя разными способами: memory, kafka (+json), parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e7299e7-2e34-4a79-848b-3b1b0fc1bb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Список файлов в директории tmp_lesson3:\n",
      "hdfs://localhost/user/root/tmp_lesson3/_spark_metadata/0\n",
      "hdfs://localhost/user/root/tmp_lesson3/part-00000-1a050473-7ac2-4ed3-b083-f8c1e8aa65b0-c000.snappy.parquet\n",
      "Содержимое файла hdfs://localhost/user/root/tmp_lesson3/part-00000-1a050473-7ac2-4ed3-b083-f8c1e8aa65b0-c000.snappy.parquet:\n",
      "PAR1\u0015\u0000\u0015*\u0015.\u0015¯ÎÄÙ\n",
      "\u001c",
      "\u0015\u0006\u0015\u0000\u0015\u0006\u0000\u0000\u0015P\u0002\u0000\u0000\u0000\u0003\u0007\u0001\u0000\u0000\u0000a\u0001\u0000\u0000\u0000b\u0001\u0000\u0000\u0000c\u0015\u0000\u0015$\u0015(\u0015ÈÝò\u0005\u001c",
      "\u0015\u0006\u0015\u0000\u0015\u0006\u0000\u0000\u0012D\u0002\u0000\u0000\u0000\u0003\u0007\u0002\u0000\u0000\u0000\u0004\u0000\u0000\u0000\u0000\u0000\u0015\u0000\u0015<\u00158\u0015ÙÄöÓ\u0005\u001c",
      "\u0015\u0006\u0015\u0000\u0015\u0006\u0000\u0000\u001e",
      "\u0018\u0002\u0000\u0000\u0000\u0003\u0007\u0000\n",
      "\u0001<\u0001\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0002\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0019\u0011\u0002\u0019\u0018\u0001a\u0019\u0018\u0001c\u0015\u0002\u0019\u0016\u0000\u0000\u0019\u0011\u0002\u0019\u0018\u0004\u0002\u0000\u0000\u0000\u0019\u0018\u0000\u0000\u0000\u0015\u0002\u0019\u0016\u0000\u0000\u0019\u0011\u0002\u0019\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0019\u0002\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0015\u0002\u0019\u0016\u0000\u0000\u0019\u001c",
      "\u0015\\\u0016\u0000\u0000\u0000\u0019\u001c",
      "\u0016d\u0015V\u0016\u0000\u0000\u0000\u0019\u001c",
      "\u0016º\u0001\u0015f\u0016\u0000\u0000\u0000\u0015\u0002\u0019LH\f",
      "spark_schema\u0015\u0006\u0000\u0015\f",
      "%\u0002column_1%\u0000L\u001c",
      "\u0000\u0000\u0000\u0015\u0002%\u0002column_2\u0000\u0015\u0004%\u0002\u0018\u0006offset\u0000\u0016\u0006\u0019\u001c",
      "\u0019<\u001c",
      "\u0015\f",
      "\u00195\u0000\u0019column_1\u0015\u0002\u0016\u0006\u0016X\u0016\\<6\u0000(\u0001c\u0018\u0001a\u0000\u0019\u001c",
      "\u0015\u0000\u0015\u0000\u0015\u0002\u0000\u0000\u0016®\u0003\u0015\u0014\u0016 \u0002\u0015\"\u0000&d\u001c",
      "\u0015\u0002\u00195\u0000\u0019column_2\u0015\u0002\u0016\u0006\u0016R\u0016V&d<\u0018\u0000\u0000\u0000\u0018\u0004\u0002\u0000\u0000\u0000\u0016\u0000(\u0000\u0000\u0000\u0018\u0004\u0002\u0000\u0000\u0000\u0000\u0019\u001c",
      "\u0015\u0000\u0015\u0000\u0015\u0002\u0000\u0000\u0016Â\u0003\u0015\u0014\u0016Â\u0002\u0015.\u0000&º\u0001\u001c",
      "\u0015\u0004\u00195\u0000\u0019\u0018\u0006offset\u0015\u0002\u0016\u0006\u0016j\u0016f&º\u0001<\u0002\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0016\u0000\u0002\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0019\u001c",
      "\u0015\u0000\u0015\u0000\u0015\u0002\u0000\u0000\u0016Ö\u0003\u0015\u0016\u0016ð\u0002\u0015>\u0000\u0016\u0002\u0016\u0006\u0016\u0002\u0014\u0000\u0000\u0019,\u0018\u0018org.apache.spark.version\u0018\u00053.3.1\u0000\u0018)org.apache.spark.sql.parquet.row.metadata\u0018ß\u0001{\"type\":\"struct\",\"fields\":[{\"name\":\"column_1\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"column_2\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"offset\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}}]}\u0000\u0018Jparquet-mr version 1.12.2 (build 77e30c8093386ec52c3cfa6c34b7ef3321322c94)\u0019<\u001c",
      "\u0000\u0000\u001c",
      "\u0000\u0000\u001c",
      "\u0000\u0000\u0000Ñ\u0002\u0000\u0000PAR1\n",
      "Stopping stream: <pyspark.sql.streaming.StreamingQuery object at 0x7d604ef35300>\n"
     ]
    }
   ],
   "source": [
    "# читаем топик кафки\n",
    "df = read_stream(spark, topic_name, schema, kf)\n",
    "# записываем в sink - parquet\n",
    "df = sink(df, path=sink_path, form='parquet', checkpoint=checkpoint)\n",
    "# немного подождём - процесс записи в hdfs небыстрый\n",
    "time.sleep(5)\n",
    "# проверяем sink в parquet:\n",
    "files = hdfs.ls(sink_path, return_paths=True, recursive=True)\n",
    "# проверяем sink в parquet:\n",
    "for file in files:\n",
    "    if file.endswith('.parquet'):\n",
    "        hdfs.cat(file)\n",
    "# останавливаем потоки\n",
    "stop_all_streams(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce1df908-653c-4106-a4d9-99980e1a7f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Такого топика не существует: tmp_lesson3\n",
      "Создаём топик: tmp_lesson3\n",
      "Топик успешно создан: tmp_lesson3\n",
      "{a, 2, 0}\n",
      "{b, 4, 1}\n",
      "{c, 8, 2}\n",
      "Stopping stream: <pyspark.sql.streaming.StreamingQuery object at 0x7d604e612320>\n"
     ]
    }
   ],
   "source": [
    "# читаем топик кафки\n",
    "df = read_stream(spark, topic_name, schema, kf)\n",
    "# записываем в sink - kafka\n",
    "df = sink(df, path=sink_path, form='kafka', checkpoint=sink_path)\n",
    "# проверяем sink в kafka:\n",
    "kf.get(name=sink_path, from_beginning=True)\n",
    "# останавливаем потоки\n",
    "stop_all_streams(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa688a03-b3eb-43d2-abba-4220ef0c115b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Такого топика не существует: tmp_lesson3_json\n",
      "Создаём топик: tmp_lesson3_json\n",
      "Топик успешно создан: tmp_lesson3_json\n",
      "{\"column_1\":\"a\",\"column_2\":2,\"offset\":0}\n",
      "{\"column_1\":\"b\",\"column_2\":4,\"offset\":1}\n",
      "{\"column_1\":\"c\",\"column_2\":8,\"offset\":2}\n",
      "Stopping stream: <pyspark.sql.streaming.StreamingQuery object at 0x7d604e612a10>\n"
     ]
    }
   ],
   "source": [
    "# читаем топик кафки\n",
    "df = read_stream(spark, topic_name, schema, kf)\n",
    "# записываем в sink - kafka\n",
    "df = sink(df, path=sink_path+'_json', form='kafka', checkpoint=sink_path+'_json', json=True)\n",
    "# проверяем sink в kafka:\n",
    "kf.get(name=sink_path+'_json', from_beginning=True)\n",
    "# останавливаем потоки\n",
    "stop_all_streams(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0df70c87-f2d1-4241-bd24-49893caec6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------+\n",
      "|column_1|column_2|offset|\n",
      "+--------+--------+------+\n",
      "|       a|       2|     0|\n",
      "|       b|       4|     1|\n",
      "|       c|       8|     2|\n",
      "+--------+--------+------+\n",
      "\n",
      "Stopping stream: <pyspark.sql.streaming.StreamingQuery object at 0x7d604ef37760>\n"
     ]
    }
   ],
   "source": [
    "# читаем топик кафки\n",
    "df = read_stream(spark, topic_name, schema, kf)\n",
    "# записываем в sink - memory\n",
    "df = sink(df, path=sink_path, form='memory')\n",
    "# проверяем sink в memory:\n",
    "time.sleep(5)\n",
    "spark.sql(f\"select * from {sink_path}\").show()\n",
    "# останавливаем потоки\n",
    "stop_all_streams(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71a0feb1-11d1-42f7-bf3a-19e01bbd172b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Топик успешно удален: lesson3\n",
      "Топик успешно удален: tmp_lesson3\n",
      "Топик успешно удален: tmp_lesson3_json\n",
      "Файл успешно удалён: /tmp\n",
      "Файл успешно удалён: /user/root\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# убираем за собой\n",
    "stop_all_streams(spark)\n",
    "kf.rm(topic_name) # удаляем топики кафки\n",
    "kf.rm(sink_path)\n",
    "kf.rm(sink_path+'_json')\n",
    "hdfs.rm('/tmp') # чистим временные данные\n",
    "hdfs.rm('/user/root') # чистим чекпойнты"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
