{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78a2bd5c-bf93-4198-9ee2-4e512d30ff6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/06 18:45:50 WARN Utils: Your hostname, alex-pc resolves to a loopback address: 127.0.1.1; using 192.168.1.70 instead (on interface wlan0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Топик уже существует: lesson3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StringType, IntegerType\n",
    "from pyspark.sql.functions import from_json, to_json, col, struct\n",
    "import time\n",
    "import signal\n",
    "from typing import Union\n",
    "\n",
    "from tools_kafka import Kafka\n",
    "from tools_pyspark_hdfs import Spark_HDFS as HDFS\n",
    "from tools_pyspark import stop_all_streams, sink, console_output\n",
    "\n",
    "def read_stream(spark, topic_name, schema, kf):\n",
    "    \"\"\"Возвращает стрим топика кафки\"\"\"\n",
    "    return spark.readStream \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", kf.SERVERS) \\\n",
    "        .option(\"subscribe\", topic_name) \\\n",
    "        .option(\"startingOffsets\", \"earliest\") \\\n",
    "        .option(\"maxOffsetsPerTrigger\", \"5\") \\\n",
    "        .load() \\\n",
    "        .select(F.from_json(F.col(\"value\").cast(\"String\"), schema).alias(\"value\"), \"offset\") \\\n",
    "        .select(\"value.*\", \"offset\")\n",
    "\n",
    "spark = SparkSession.builder.appName(\"my_spark\").getOrCreate()\n",
    "\n",
    "kf = Kafka()\n",
    "hdfs = HDFS(spark)\n",
    "\n",
    "topic_name = 'lesson3'\n",
    "sink_path = f'tmp_{topic_name}'  # в случае с kafka это название топика, где хранится информация\n",
    "checkpoint = f'sink_{sink_path}'\n",
    "\n",
    "schema = StructType() \\\n",
    "    .add(\"column_1\", StringType()) \\\n",
    "    .add(\"column_2\", IntegerType())\n",
    "\n",
    "kf.add('lesson3') # создаём топик кафки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63570425-aa3c-4fd4-a70d-de32e3447808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"column_1\":\"a\",\"column_2\":2}\n",
      "{\"column_1\":\"b\",\"column_2\":4}\n",
      "{\"column_1\":\"c\",\"column_2\":8}\n",
      "Stopping stream: <pyspark.sql.streaming.StreamingQuery object at 0x7938980db940>\n"
     ]
    }
   ],
   "source": [
    "# Читаем файл и переносим его в топик кафки\n",
    "stream = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .schema(schema) \\\n",
    "    .csv(\"/data/\") \\\n",
    "    .selectExpr(\"CAST(null AS STRING) as key\", \n",
    "                \"CAST(to_json(struct(*)) AS STRING) as value\") \\\n",
    "    .writeStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kf.SERVERS) \\\n",
    "    .option(\"topic\", topic_name) \\\n",
    "    .option(\"checkpointLocation\", \"checkpoints/stream_read_write\") \\\n",
    "    .start()\n",
    "\n",
    "# выводим содержимое топика\n",
    "kf.get(topic_name) \n",
    "# остановим потоки\n",
    "stop_all_streams(spark) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e514ce1d-aa5a-4afc-b059-220a40b380ca",
   "metadata": {},
   "source": [
    "### Попробуем сохранить данные в sink тремя разными способами: memory, kafka (+josn), parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e7299e7-2e34-4a79-848b-3b1b0fc1bb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Нет файлов в директории tmp_lesson3\n",
      "Stopping stream: <pyspark.sql.streaming.StreamingQuery object at 0x7938981fe860>\n"
     ]
    }
   ],
   "source": [
    "# читаем топик кафки\n",
    "df = read_stream(spark, topic_name, schema, kf)\n",
    "# записываем в sink - parquet\n",
    "df = sink(df, path=sink_path, form='parquet', checkpoint=checkpoint)\n",
    "# проверяем sink в parquet:\n",
    "files = hdfs.ls(sink_path, return_paths=True, recursive=True)\n",
    "# проверяем sink в parquet:\n",
    "for file in files:\n",
    "    if file.endswith('.parquet'):\n",
    "        hdfs.cat(file)\n",
    "# останавливаем потоки\n",
    "stop_all_streams(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce1df908-653c-4106-a4d9-99980e1a7f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Такого топика не существует: tmp_lesson3\n",
      "Создаём топик: tmp_lesson3\n",
      "Топик успешно создан: tmp_lesson3\n",
      "{a, 2, 0}\n",
      "{b, 4, 1}\n",
      "{c, 8, 2}\n",
      "Stopping stream: <pyspark.sql.streaming.StreamingQuery object at 0x7938980d8f70>\n"
     ]
    }
   ],
   "source": [
    "# читаем топик кафки\n",
    "df = read_stream(spark, topic_name, schema, kf)\n",
    "# записываем в sink - kafka\n",
    "df = sink(df, path=sink_path, form='kafka', checkpoint=sink_path)\n",
    "# проверяем sink в kafka:\n",
    "kf.get(name=sink_path, from_beginning=True)\n",
    "# останавливаем потоки\n",
    "stop_all_streams(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa688a03-b3eb-43d2-abba-4220ef0c115b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"column_1\":\"a\",\"column_2\":2,\"offset\":0}\n",
      "{\"column_1\":\"b\",\"column_2\":4,\"offset\":1}\n",
      "{\"column_1\":\"c\",\"column_2\":8,\"offset\":2}\n",
      "Stopping stream: <pyspark.sql.streaming.StreamingQuery object at 0x7938980da7a0>\n"
     ]
    }
   ],
   "source": [
    "# читаем топик кафки\n",
    "df = read_stream(spark, topic_name, schema, kf)\n",
    "# записываем в sink - kafka\n",
    "df = sink(df, path=sink_path+'_json', form='kafka', checkpoint=sink_path+'_json', json=True)\n",
    "# проверяем sink в kafka:\n",
    "kf.get(name=sink_path+'_json', from_beginning=True)\n",
    "# останавливаем потоки\n",
    "stop_all_streams(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0df70c87-f2d1-4241-bd24-49893caec6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------+\n",
      "|column_1|column_2|offset|\n",
      "+--------+--------+------+\n",
      "|       a|       2|     0|\n",
      "|       b|       4|     1|\n",
      "|       c|       8|     2|\n",
      "+--------+--------+------+\n",
      "\n",
      "Stopping stream: <pyspark.sql.streaming.StreamingQuery object at 0x7938981fddb0>\n"
     ]
    }
   ],
   "source": [
    "# читаем топик кафки\n",
    "df = read_stream(spark, topic_name, schema, kf)\n",
    "# записываем в sink - memory\n",
    "df = sink(df, path=sink_path, form='memory')\n",
    "# проверяем sink в memory:\n",
    "time.sleep(5)\n",
    "spark.sql(f\"select * from {sink_path}\").show()\n",
    "# останавливаем потоки\n",
    "stop_all_streams(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71a0feb1-11d1-42f7-bf3a-19e01bbd172b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Топик успешно удален: lesson3\n",
      "Топик успешно удален: tmp_lesson3\n",
      "Топик успешно удален: tmp_lesson3_json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# убираем за собой\n",
    "stop_all_streams(spark)\n",
    "kf.rm(topic_name) # удаляем топики кафки\n",
    "kf.rm(sink_path)\n",
    "kf.rm(sink_path+'_json')\n",
    "hdfs.rm('/tmp') # чистим временные данные\n",
    "hdfs.rm('/user/root') # чистим чекпойнты"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
