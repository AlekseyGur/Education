{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac93313c-5856-4402-8188-5f52cd39585a",
   "metadata": {},
   "source": [
    "# Предсказания по модели на PySpark\n",
    "\n",
    "Исходные данные разделим на две части: в кассандру и кафку. При этом добавим столец id, чтобы объединять по нему после. В конце сделаем стрим из кафки, где будем одновременно соединять разделённый датасет и делать предсказания по заранее сохранённой модели машинного обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc6ec2da-41a1-4098-8c93-38031ffbfe86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /data\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -rm -r /data\n",
    "! hdfs dfs -mkdir -p /data/diabet\n",
    "! hdfs dfs -put diabetes.csv /data/diabet/db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78a2bd5c-bf93-4198-9ee2-4e512d30ff6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/11 14:23:28 WARN Utils: Your hostname, alex resolves to a loopback address: 127.0.1.1; using 192.168.1.70 instead (on interface wlan0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+-------+-------------+-------------+\n",
      "| id|pregnancies|glucose|bloodpressure|skinthickness|\n",
      "+---+-----------+-------+-------------+-------------+\n",
      "|  0|          6|    148|           72|           35|\n",
      "|  1|          1|     85|           66|           29|\n",
      "|  2|          8|    183|           64|            0|\n",
      "|  3|          1|     89|           66|           23|\n",
      "|  4|          0|    137|           40|           35|\n",
      "+---+-----------+-------+-------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---+-------+----+------------------------+---+\n",
      "| id|insulin| bmi|diabetespedigreefunction|age|\n",
      "+---+-------+----+------------------------+---+\n",
      "|  0|      0|33.6|                   0.627| 50|\n",
      "|  1|      0|26.6|                   0.351| 31|\n",
      "|  2|      0|23.3|                   0.672| 32|\n",
      "|  3|     94|28.1|                   0.167| 21|\n",
      "|  4|    168|43.1|                   2.288| 33|\n",
      "+---+-------+----+------------------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--master local --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1,com.datastax.spark:spark-cassandra-connector_2.12:3.2.0 pyspark-shell'\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StringType, IntegerType, DoubleType, BooleanType\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from cassandra.cluster import Cluster # https://github.com/datastax/python-driver\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.sql.functions import col as Fcol\n",
    "from pyspark.ml.feature import OneHotEncoder, VectorAssembler, CountVectorizer, StringIndexer, IndexToString\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "from tools_pyspark_hdfs import Spark_HDFS as HDFS\n",
    "from tools_kafka import Kafka\n",
    "from tools_pyspark import stop_all_streams, sink, read_stream_kafka, console_stream, console_clear, console_show\n",
    "from tools_cassandra import schema_cassandra_table\n",
    "\n",
    "# В качестве датасета выбран: https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database\n",
    "\n",
    "spark = SparkSession.builder.appName(\"my_spark\").getOrCreate()\n",
    "hdfs = HDFS(spark)\n",
    "kf = Kafka()\n",
    "topic_name = 'lesson8'\n",
    "\n",
    "# колонка Pregnancies принудительно делалась категориальной\n",
    "schema = StructType() \\\n",
    "    .add(\"Pregnancies\", StringType()) \\\n",
    "    .add(\"Glucose\", IntegerType()) \\\n",
    "    .add(\"BloodPressure\", IntegerType()) \\\n",
    "    .add(\"SkinThickness\", IntegerType()) \\\n",
    "    .add(\"Insulin\", IntegerType()) \\\n",
    "    .add(\"BMI\", DoubleType()) \\\n",
    "    .add(\"DiabetesPedigreeFunction\", DoubleType()) \\\n",
    "    .add(\"Age\", IntegerType()) \\\n",
    "    .add(\"Outcome\", IntegerType())\n",
    "\n",
    "target_column_name = 'Outcome' # это целевая колонка, которую надо предсказать\n",
    "\n",
    "df = spark.read.csv(\n",
    "    \"/data/diabet/db\",\n",
    "    header=True,\n",
    "    schema=schema\n",
    ")\n",
    "\n",
    "for col in df.columns:  # часть данных будем писать в касандру, а она понимает только строчные буквы \n",
    "    df = df.withColumnRenamed(col, col.lower())\n",
    "\n",
    "target_column_name = 'Outcome' # это целевая колонка, которую надо предсказать\n",
    "categorical_columns = [f.name for f in schema.fields if 'StringType' in str(f.typeName)]\n",
    "number_columns = list(set(schema.fieldNames()) - set(categorical_columns))\n",
    "number_columns.remove(target_column_name) # убираем целевую колонку\n",
    "\n",
    "df = df.select(\"*\").withColumn(\"id\", monotonically_increasing_id())\n",
    "\n",
    "# Разделим фрейм на два по колонкам. При этом в каждом оставим колонку id,\n",
    "df_kafka = df.select('id', 'pregnancies', 'glucose', 'bloodpressure', 'skinthickness') # это запишем в кафку\n",
    "df_cassandra = df.select('id', 'insulin', 'bmi', 'diabetespedigreefunction', 'age') # это в кассандру\n",
    "df_kafka.show(5)\n",
    "df_cassandra.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a449936e-0bf1-4eb8-9c97-7c1f104d624d",
   "metadata": {},
   "source": [
    "## Загрузим данные в таблицу кассандры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0fbb39e-bab6-4051-a667-ab2131cf6935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+------------------------+-------+\n",
      "| id|age|bmi|diabetespedigreefunction|insulin|\n",
      "+---+---+---+------------------------+-------+\n",
      "| 52| 30| 24|                       0|     23|\n",
      "|311| 22| 39|                       0|    148|\n",
      "|732| 24| 44|                       0|    120|\n",
      "|472| 22| 38|                       0|      0|\n",
      "|283| 47| 30|                       0|      0|\n",
      "+---+---+---+------------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Создадим нужную таблицу\n",
    "cluster = Cluster(['localhost'])\n",
    "session = cluster.connect()\n",
    "\n",
    "session.execute(\"DROP KEYSPACE IF EXISTS lesson8;\")\n",
    "session.execute(\"CREATE KEYSPACE IF NOT EXISTS lesson8 WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor': 1 };\")\n",
    "session.execute(\"USE lesson8;\")\n",
    "session.execute(f\"CREATE TABLE IF NOT EXISTS db ( {schema_cassandra_table(df_cassandra, primary_key='id')} );\")\n",
    "\n",
    "cluster.shutdown()\n",
    "\n",
    "# запишем фрейм в таблицу\n",
    "df_cassandra.write \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .options(table=\"db\", keyspace=\"lesson8\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .save()\n",
    "\n",
    "# для контроля прочитаем таблицу\n",
    "spark.read \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .options(table=\"db\", keyspace=\"lesson8\") \\\n",
    "    .load() \\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c977d34f-f996-4500-82a1-1de2775876f8",
   "metadata": {},
   "source": [
    "## Загрузим данные в кафку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12118e21-b132-43e7-b41f-a665ac558e7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\"id\":0,\"pregnancies\":\"6\",\"glucose\":148,\"bloodpressure\":72,\"skinthickness\":35}',\n",
       " '{\"id\":1,\"pregnancies\":\"1\",\"glucose\":85,\"bloodpressure\":66,\"skinthickness\":29}',\n",
       " '{\"id\":2,\"pregnancies\":\"8\",\"glucose\":183,\"bloodpressure\":64,\"skinthickness\":0}',\n",
       " '{\"id\":3,\"pregnancies\":\"1\",\"glucose\":89,\"bloodpressure\":66,\"skinthickness\":23}',\n",
       " '{\"id\":4,\"pregnancies\":\"0\",\"glucose\":137,\"bloodpressure\":40,\"skinthickness\":35}']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# создадим топик кафки\n",
    "if topic_name not in kf.ls():\n",
    "    kf.add(topic_name)\n",
    "\n",
    "# отправим данные в топик кафки\n",
    "stream = df_kafka \\\n",
    "        .selectExpr(\"CAST(null AS STRING) as key\", \n",
    "                    \"CAST(to_json(struct(*)) AS STRING) as value\") \\\n",
    "        .write \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", kf.SERVERS) \\\n",
    "        .option(\"topic\", topic_name) \\\n",
    "        .option(\"checkpointLocation\", \"checkpoints/stream_read_write\") \\\n",
    "        .save()\n",
    "\n",
    "# посмотрим содержимое топика кафки\n",
    "kf.get(topic_name, return_rows=True)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179a75b6-d7ae-443f-95e8-2c1af8185e03",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Сделаем поток из кафки, к которому будут добавляться данные из кассандры\n",
    "\n",
    "На объединённых данных будет делаться предсказание наличия диабета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fec6cb6f-d196-4901-ab9d-bff6693bb4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping stream: <pyspark.sql.streaming.StreamingQuery object at 0x7141e095f010>\n",
      "+-----------+-------+-------------+-------------+-------+---+------------------------+---+------+\n",
      "|Pregnancies|Glucose|BloodPressure|SkinThickness|Insulin|BMI|DiabetesPedigreeFunction|Age|Diabet|\n",
      "+-----------+-------+-------------+-------------+-------+---+------------------------+---+------+\n",
      "|6          |148    |72           |35           |0      |33 |0                       |50 |0     |\n",
      "|1          |85     |66           |29           |0      |26 |0                       |31 |0     |\n",
      "+-----------+-------+-------------+-------------+-------+---+------------------------+---+------+\n",
      "\n",
      "Файл успешно удалён: console\n",
      "Файл успешно удалён: checkpoint\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загружаем из файла готовую модель для предсказаний  из HDFS\n",
    "pipeline_model = PipelineModel.load(\"my_LR_model\")\n",
    "\n",
    "# DataFrame для запросов к касандре с историческими данными\n",
    "df_cassandra = spark.read \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .options(table=\"db\", keyspace=\"lesson8\") \\\n",
    "    .load()\n",
    "\n",
    "#вся логика в этом foreachBatch\n",
    "def writer_logic(df, epoch_id):\n",
    "    df.persist()\n",
    "    df_cassandra.persist()\n",
    "    \n",
    "    joined = df.join(df_cassandra, \"id\", \"left\") \\\n",
    "        .select(\n",
    "            Fcol(\"pregnancies\").alias(\"Pregnancies\"),\n",
    "            Fcol(\"glucose\").alias(\"Glucose\"),\n",
    "            Fcol(\"bloodpressure\").alias(\"BloodPressure\"),\n",
    "            Fcol(\"skinthickness\").alias(\"SkinThickness\"),\n",
    "            Fcol(\"insulin\").alias(\"Insulin\"),\n",
    "            Fcol(\"bmi\").alias(\"BMI\"),\n",
    "            Fcol(\"diabetespedigreefunction\").alias(\"DiabetesPedigreeFunction\"),\n",
    "            Fcol(\"age\").alias(\"Age\"))\n",
    "\n",
    "    # колонки на предсказании и обучении должны быть одинаковыми.\n",
    "    # Поэтому делаем точно такие же преобразования с батчем,\n",
    "    # что и при обучении модели.\n",
    "    pred =  pipeline_model.transform(joined).select('Pregnancies', \n",
    "                                                    'Glucose', \n",
    "                                                    'BloodPressure', \n",
    "                                                    'SkinThickness', \n",
    "                                                    'Insulin', \n",
    "                                                    'BMI', \n",
    "                                                    'DiabetesPedigreeFunction', \n",
    "                                                    'Age', \n",
    "                                                    Fcol('category').alias('Diabet'))\n",
    "    \n",
    "    console_stream(pred)  # выводим в файловую консоль\n",
    "    \n",
    "    # Можно записать данные обратно в кассандру, чтобы по ним же тренировать модель дальше:\n",
    "    # predict_short.write \\\n",
    "    #     .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    #     .options(table=\"db\", keyspace=\"lesson8\") \\\n",
    "    #     .mode(\"append\") \\\n",
    "    #     .save()\n",
    "    \n",
    "    df_cassandra.unpersist()\n",
    "    df.unpersist()\n",
    "\n",
    "# читаем стрим из кафки\n",
    "stream_kafka = read_stream_kafka(spark, \n",
    "                                 server=kf.SERVERS, \n",
    "                                 topic_name=topic_name, \n",
    "                                 schema=df_kafka.schema, \n",
    "                                 maxOffsetsPerTrigger=1)\n",
    "\n",
    "#связываем источник Кафки и Кассандры через foreachBatch функцию\n",
    "stream = stream_kafka \\\n",
    "    .writeStream \\\n",
    "    .trigger(processingTime='1 seconds') \\\n",
    "    .foreachBatch(writer_logic) \\\n",
    "    .option(\"checkpointLocation\", \"checkpoint/lesson8\")\n",
    "\n",
    "# Запускаем поток\n",
    "stream = stream.start()\n",
    "# ждём\n",
    "time.sleep(15)\n",
    "# останавливаем поток\n",
    "stop_all_streams(spark)\n",
    "# проверяем содержимое консоли\n",
    "console_show(spark)\n",
    "# удаляем консоль и чекпойнты\n",
    "console_clear(spark)\n",
    "hdfs.rm('checkpoint')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
