{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f29b631-dbf7-424d-ab66-de4fa8d8ec47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StringType, IntegerType, TimestampType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"my_spark\").getOrCreate()\n",
    "\n",
    "#функция, чтобы выводить на консоль, вместо show()\n",
    "def console_output(df, freq):\n",
    "    return df.writeStream.format(\"console\") \\\n",
    "        .trigger(processingTime=f'{freq} seconds') \\\n",
    "        .options(truncate=False) \\\n",
    "        .start()\n",
    "\n",
    "######RATE SOURCE\n",
    "raw_rate = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"rate\") \\\n",
    "    .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "645f65c2-6f41-4a89-9578-d03a2ed9bdb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- value: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_rate.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9d7b0ac-f953-4940-800c-e9556df851a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/02 13:49:15 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-487e09a9-2146-4b3a-b3d2-2260e48945fb. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "22/12/02 13:49:15 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+---------+-----+\n",
      "|timestamp|value|\n",
      "+---------+-----+\n",
      "+---------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+\n",
      "|timestamp              |value|\n",
      "+-----------------------+-----+\n",
      "|2022-12-02 13:49:15.762|0    |\n",
      "|2022-12-02 13:49:17.762|2    |\n",
      "|2022-12-02 13:49:16.762|1    |\n",
      "|2022-12-02 13:49:18.762|3    |\n",
      "+-----------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "out = console_output(raw_rate, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62f0c70a-aa50-47c6-baca-6a8782650c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "464ee9a6-2c17-47e3-ba11-0a794737248a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/02 13:51:08 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-4382a165-6345-4f38-be38-ede37825e2aa. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "22/12/02 13:51:08 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+---------+-----+\n",
      "|timestamp|value|\n",
      "+---------+-----+\n",
      "+---------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+\n",
      "|timestamp              |value|\n",
      "+-----------------------+-----+\n",
      "|2022-12-02 13:51:08.851|0    |\n",
      "+-----------------------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+\n",
      "|timestamp              |value|\n",
      "+-----------------------+-----+\n",
      "|2022-12-02 13:51:10.851|2    |\n",
      "|2022-12-02 13:51:12.851|4    |\n",
      "+-----------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#добавляем собственный фильтр\n",
    "filtered_rate = raw_rate \\\n",
    "    .filter( F.col(\"value\")%F.lit(2)==0 )\n",
    "\n",
    "out = console_output(filtered_rate, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "558c6601-2944-4837-80af-d7f1a861547c",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "716a2f14-a569-4b36-9085-174b5910d6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/02 13:51:36 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-25ea16ce-c94a-4a1e-8113-caad192b9608. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "22/12/02 13:51:36 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+---------+-----+--------+\n",
      "|timestamp|value|my_value|\n",
      "+---------+-----+--------+\n",
      "+---------+-----+--------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+----------------------+-----+--------+\n",
      "|timestamp             |value|my_value|\n",
      "+----------------------+-----+--------+\n",
      "|2022-12-02 13:51:36.47|0    |jubilee |\n",
      "|2022-12-02 13:51:38.47|2    |not yet |\n",
      "+----------------------+-----+--------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+----------------------+-----+--------+\n",
      "|timestamp             |value|my_value|\n",
      "+----------------------+-----+--------+\n",
      "|2022-12-02 13:51:40.47|4    |not yet |\n",
      "|2022-12-02 13:51:42.47|6    |not yet |\n",
      "+----------------------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#добавляем собственные колонки\n",
    "extra_rate = filtered_rate \\\n",
    "    .withColumn(\"my_value\",\n",
    "                F.when((F.col(\"value\") % F.lit(10) == 0), F.lit(\"jubilee\"))\n",
    "                    .otherwise(F.lit(\"not yet\")))\n",
    "\n",
    "out = console_output(extra_rate, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "175bf367-6e86-416e-9763-66ffeb19b1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9319907f-c309-4944-a969-90a4723cb30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#если потеряем стрим из переменной, сможем остановить все наши стримы, получих их из спарк окружения\n",
    "def killAll():\n",
    "    for s in spark.streams.active:\n",
    "        print(f\"Stopping {s} for killAll\")\n",
    "        s.stop()\n",
    "        \n",
    "killAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6aff4751-c79e-4374-af87-bc4171758a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "######FILE SOURCE\n",
    "schema = StructType() \\\n",
    "    .add(\"column_1\", StringType()) \\\n",
    "    .add(\"column_2\", StringType())\n",
    "\n",
    "# В options-path должна быть указана директория, а не файл!\n",
    "raw_files = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"csv\") \\\n",
    "    .schema(schema) \\\n",
    "    .options(path=\"input_csv_for_stream\", header=True) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "990d482b-df28-46a1-b736-91402cd66930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/02 14:03:12 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-5b14b7e6-00a2-4518-b1e2-0d6877be164a. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "22/12/02 14:03:12 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+--------+--------+\n",
      "|column_1|column_2|\n",
      "+--------+--------+\n",
      "|a       |2       |\n",
      "|b       |4       |\n",
      "|c       |8       |\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "out = console_output(raw_files, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "52dc4722-dd1e-4939-9a5f-020d5abb8c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2d76b4a7-9ba0-4dc7-bf0e-f2958e286f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/02 14:03:47 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-bcddbd05-286c-4831-b875-5329cef50acb. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "22/12/02 14:03:47 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+--------+--------+\n",
      "|column_1|column_2|\n",
      "+--------+--------+\n",
      "|a       |2       |\n",
      "|b       |4       |\n",
      "|c       |8       |\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#по одному\n",
    "raw_files = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"csv\") \\\n",
    "    .schema(schema) \\\n",
    "    .options(path=\"input_csv_for_stream\", header=True, maxFilesPerTrigger=1) \\\n",
    "    .load()\n",
    "\n",
    "out = console_output(raw_files, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "22c5b4e9-6a30-4a9a-aa46-ff3456f303fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a989c2eb-b6ff-457a-9a8d-85af2c48a39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/02 14:08:37 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-c30fb624-8c19-423f-b188-ff26911ea648. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "22/12/02 14:08:37 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+--------+--------+--------+------------------+\n",
      "|column_1|column_2|column_3|column_4          |\n",
      "+--------+--------+--------+------------------+\n",
      "|a       |2       |1       |7.38905609893065  |\n",
      "|b       |4       |1       |54.598150033144236|\n",
      "|c       |8       |1       |2980.9579870417283|\n",
      "+--------+--------+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# добавляем свою колонку\n",
    "extra_files = raw_files \\\n",
    "    .withColumn(\"column_3\", F.length(F.col(\"column_2\"))) \\\n",
    "    .withColumn(\"column_4\", F.exp(\"column_2\"))\n",
    "\n",
    "out = console_output(extra_files, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bce3a4c1-35f2-43d4-8094-9017c7f010de",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
