{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f0968e-ceca-4db2-9916-2b2610225d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StringType, FloatType, IntegerType\n",
    "from pyspark.sql.functions import from_json, to_json, col, struct\n",
    "from typing import Union\n",
    "import subprocess\n",
    "\n",
    "\n",
    "topic_name = 'lesson3' # название топика для кафки\n",
    "checkpoints = '/tmp/checkpoints/stream_read_write' # место хранения чекпойнтов внутри hdfs\n",
    "\n",
    "\n",
    "def console_output(df, freq:int=5):\n",
    "    \"\"\"Воводит содержимое потока spark в консоль\n",
    "    :param df: количество\n",
    "    :param freq: периодичность вывода. Кол-во секунд\n",
    "    \"\"\"\n",
    "    res = df.writeStream \\\n",
    "        .format(\"console\") \\\n",
    "        .trigger(processingTime=f'{freq} seconds') \\\n",
    "        .outputMode(\"complete\") \\\n",
    "        .options(truncate=False) \\\n",
    "        .start()\n",
    "    \n",
    "    return res\n",
    "\n",
    "def stop_all_streams(spark):\n",
    "    \"\"\"Останаваливает все стримы\"\"\"\n",
    "    streams = spark.streams.active\n",
    "    if not streams:\n",
    "        print('Нет активных потоков')\n",
    "    else:\n",
    "        for active_stream in spark.streams.active:\n",
    "            print(f'Stopping stream: {active_stream}')\n",
    "            active_stream.stop()\n",
    "\n",
    "\n",
    "def console_output(df, freq:int=5):\n",
    "    \"\"\"Воводит содержимое потока spark в консоль\n",
    "    :param df: количество\n",
    "    :param freq: периодичность вывода. Кол-во секунд\n",
    "    \"\"\"\n",
    "    return df.writeStream \\\n",
    "        .format(\"console\") \\\n",
    "        .trigger(processingTime=f'{freq} seconds') \\\n",
    "        .options(truncate=False) \\\n",
    "        .start()\n",
    "\n",
    "class Spark_HDFS():\n",
    "    SC = None # spark context\n",
    "    FS = None # hdfs FileSystem in spark\n",
    "    Path = None # функция получения java идентификаторов файлов в hdfs\n",
    "    \n",
    "    def __init__(self, spark):\n",
    "        self.SC = spark.sparkContext\n",
    "        self.Path = self.SC._jvm.org.apache.hadoop.fs.Path\n",
    "        self.FS = (self.SC._jvm.org\n",
    "                  .apache.hadoop\n",
    "                  .fs.FileSystem\n",
    "                  .get(self.SC._jsc.hadoopConfiguration()) )\n",
    "    \n",
    "    def ls(self, path:str, recursive:bool=True) -> None:\n",
    "        \"\"\"Показывает список файлов в директории HDFS по заданному пути\n",
    "        :param path: путь к директории внутри HDFS\n",
    "        :param recursive: удалить рекурсивно\n",
    "        :return: Ture (если успешно) и False (если проблема)\n",
    "        \"\"\"\n",
    "        status = self.FS.listStatus(self.Path(path))\n",
    "        for fileStatus in status:\n",
    "            print(fileStatus.getPath())\n",
    "    \n",
    "    def rm(self, path:str, recursive:bool=True) -> bool:\n",
    "        \"\"\"Удаляет файл в HDFS по заданному пути\n",
    "        :param path: путь к файлу внутри HDFS\n",
    "        :param recursive: удалить рекурсивно\n",
    "        :return: Ture (если успешно) и False (если проблема)\n",
    "        \"\"\"\n",
    "        res = self.FS.delete(self.SC._jvm.org.apache.hadoop.fs.Path(path), True)\n",
    "        msg = f'Файл успешно удалён: {path}' if res else f'Ну удалось удалить файл: {path}'\n",
    "        return res\n",
    "\n",
    "    def put(self, local_file:str, hdfs_file:str, delSrc:bool=False, overwrite:bool=True) -> bool:\n",
    "        \"\"\"Копирует локальный файл в HDFS\n",
    "        :param local_file: путь к локальному файлу \n",
    "        :param hdfs_file: путь к файлу внутри HDFS\n",
    "        :param delSrc: удалить файл-источник после успешной передачи\n",
    "        :param overwrite: перезаписать файл, если он уже существует\n",
    "        :return: Ture (если успешно) и False (если проблема)\n",
    "        \"\"\" \n",
    "        res = True\n",
    "        try:\n",
    "            self.FS.copyFromLocalFile(delSrc, \n",
    "                                 overwrite, \n",
    "                                 self.Path(local_file), \n",
    "                                 self.Path(hdfs_file)\n",
    "                                )\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            res = False\n",
    "\n",
    "        msg = f'Файл успешно скопирован: {local_file}' if res else f'Ну удалось скопировать файл: {local_file}'\n",
    "        print(msg)\n",
    "        return res\n",
    "    \n",
    "\n",
    "class Kafka():\n",
    "    HOMEDIR = '/home/hsk/kafka/bin'  # путь к папке с исполняемыми файлами Кафки: kafka-topics.sh...\n",
    "    SERVERS = 'localhost:9092'  # адреса серверов с портами через запятую\n",
    "    \n",
    "    def __init__(self, servers:str='', homedir:str=''):\n",
    "        if servers:\n",
    "            self.SERVERS = servers\n",
    "        if homedir:\n",
    "            self.HOMEDIR = homedir\n",
    "\n",
    "    def _decode_bash_output(self, output:bytes) -> list:\n",
    "        \"\"\"Преобразует вывод bash команд в список\n",
    "        :param name: название топика\n",
    "        :return: список со строками, которые вывелись в bash\n",
    "        \"\"\"\n",
    "        return list(filter(len, output.decode('utf-8').split('\\n')))\n",
    "\n",
    "    def topic_create(self, name:Union[str, list], retention:float=120) -> bool:\n",
    "        \"\"\"Создаёт топик в kafka\n",
    "        :param name: название топика\n",
    "        :param retention: время жизни информации в топике, секунды\n",
    "        :return: Ture (если успешно) и False (если проблема)\n",
    "        \"\"\"\n",
    "        retention *= 1000\n",
    "        retention = int(retention)\n",
    "        \n",
    "        names = name\n",
    "        if isinstance(name, str):\n",
    "            names = [name]\n",
    "        \n",
    "        result = True\n",
    "        for name in names:\n",
    "            if name in self.topic_list():\n",
    "                print(f'Топик уже существует: {name}')\n",
    "                continue\n",
    "\n",
    "            bashCommand = f'{self.HOMEDIR}/kafka-topics.sh --bootstrap-server {self.SERVERS} ' +\\\n",
    "                          f'--create --topic {name} --replication-factor 1 --partitions 1 ' +\\\n",
    "                          f'--config retention.ms={retention}'\n",
    "            \n",
    "            process = subprocess.Popen(bashCommand.split(), stdout=subprocess.PIPE)\n",
    "            output, error = process.communicate()\n",
    "            output = self._decode_bash_output(output)\n",
    "\n",
    "            if name not in self.topic_list():\n",
    "                if output:\n",
    "                    print(output)\n",
    "                print(f'Не удалось создать топик: {name}')\n",
    "                result = False\n",
    "            else:\n",
    "                print(f'Топик успешно создан: {name}')\n",
    "            \n",
    "            if error:\n",
    "                if output:\n",
    "                    print(output)\n",
    "                result = False\n",
    "\n",
    "        return result\n",
    "\n",
    "    def topic_list(self) -> list:\n",
    "        \"\"\"Список топиков в kafka\n",
    "        :return: список\n",
    "        \"\"\"\n",
    "        bashCommand = f'{self.HOMEDIR}/kafka-topics.sh --bootstrap-server {self.SERVERS} --list'\n",
    "\n",
    "        process = subprocess.Popen(bashCommand.split(), stdout=subprocess.PIPE)\n",
    "        output, error = process.communicate()\n",
    "        output = self._decode_bash_output(output)\n",
    "        if error or not len(output):\n",
    "            return []\n",
    "\n",
    "        return output\n",
    "\n",
    "    def topic_delete(self, name:Union[str, list]) -> bool:\n",
    "        \"\"\"Удаляет топик в kafka\n",
    "        :param name: название топика или список из названий\n",
    "        :return: Ture - если успешно созданы все топики\n",
    "                 False - если была ошибка хотя бы с одним\n",
    "                        (остуствие топика не считается ошибкой)\n",
    "        \"\"\"\n",
    "        names = name\n",
    "        if isinstance(name, str):\n",
    "            names = [name]\n",
    "            \n",
    "        result = True\n",
    "        for name in names:\n",
    "            if name not in self.topic_list():\n",
    "                print(f'Такого топика не существует: {name}')\n",
    "                continue\n",
    "\n",
    "            bashCommand = f'{self.HOMEDIR}/kafka-topics.sh --bootstrap-server {self.SERVERS} ' +\\\n",
    "                          f'--delete --topic {name}'\n",
    "            \n",
    "            process = subprocess.Popen(bashCommand.split(), stdout=subprocess.PIPE)\n",
    "            output, error = process.communicate()\n",
    "            output = self._decode_bash_output(output)\n",
    "\n",
    "            if name in self.topic_list():\n",
    "                if output:\n",
    "                    print(output)\n",
    "                print(f'Не удалось удалить топик: {name}')\n",
    "                result = False\n",
    "            else:\n",
    "                print(f'Топик успешно удален: {name}')\n",
    "\n",
    "            if error:\n",
    "                if output:\n",
    "                    print(output)\n",
    "                result = False\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def topic_read(self, name:Union[str, list], from_beginning:bool=False, timeout:float=1) -> bool:\n",
    "        \"\"\"Читает все текущие сообщения в топике kafka. После окончания не продолжает следить за потоком\n",
    "        :param name: название топика или список из названий\n",
    "        :return: Ture - если успешно созданы все топики\n",
    "                 False - если была ошибка хотя бы с одним\n",
    "                        (остуствие топика не считается ошибкой)\n",
    "        \"\"\"\n",
    "        timeout *= 1000\n",
    "        timeout = int(timeout)\n",
    "        if name not in self.topic_list():\n",
    "            print(f'Такого топика не существует: {name}')\n",
    "\n",
    "        bashCommand = f'{self.HOMEDIR}/kafka-console-consumer.sh --bootstrap-server {self.SERVERS} ' +\\\n",
    "                      f'--topic {name} --timeout-ms {timeout}  2>/dev/null'\n",
    "\n",
    "        if from_beginning:\n",
    "            bashCommand += ' --from-beginning'\n",
    "        \n",
    "        process = subprocess.Popen(bashCommand.split(), stdout=subprocess.PIPE)\n",
    "        output, error = process.communicate()\n",
    "        output = self._decode_bash_output(output)\n",
    "        print(output)\n",
    "    \n",
    "    \n",
    "spark = SparkSession.builder.appName(\"my_spark\").getOrCreate()\n",
    "hdfs = Spark_HDFS(spark)\n",
    "kf = Kafka()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df01285-efbb-4cbf-ac05-04c549233aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# копируем файл в hdfs с локального диска (для тестов один!)\n",
    "hdfs.put(\"db.csv\", \"/data/db.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc81380-b320-4185-baf3-92bfb3233f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs.ls('/data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254d3b12-de44-4ec1-90fa-bf8c55ed2fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# читаем из локальной системы\n",
    "df = spark.read.csv(path=\"/data/db.csv\", sep=\",\", header=True)\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ccf352-4023-4f1f-99a4-9fbae4f89413",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4934398-d855-4041-a498-6f60f4d77957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# разделяем выйл на партифии, чтобы считать всё\n",
    "df.repartition(2).write.csv(path=\"/data/db_re\", sep=\",\", header=True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe480c5e-a6af-495b-aef9-b2f1bc68b228",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType() \\\n",
    "    .add(\"column_1\", StringType()) \\\n",
    "    .add(\"column_2\", IntegerType())\n",
    "\n",
    "raw_files = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"csv\") \\\n",
    "    .schema(schema) \\\n",
    "    .options(path=\"/data/db_re\",\n",
    "             header=True,\n",
    "             maxFilesPerTrigger=1) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1265312a-0878-49c6-ab07-727b9a2f0743",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = console_output(raw_files, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1cd9a3-d7ad-4375-81e5-8da678137659",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_all_streams(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d87521-46bf-4385-bf64-6667acd4e089",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4f1bb9-ae68-49f5-99a6-86360b73e833",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf.topic_create(name=topic_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad88c0f7-07cb-4cc3-ba3d-316e4efba6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf.topic_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4461cf-3faa-403f-8254-5e4ac001552e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удалим чекпоинты, чтобы начать всё заново\n",
    "hdfs.rm(checkpoints)\n",
    "\n",
    "# https://stackoverflow.com/questions/44584476/do-we-need-to-checkpoint-both-readstream-and-writestream-of-kafka-in-spark-struc\n",
    "# https://sparkbyexamples.com/spark/spark-streaming-with-kafka/\n",
    "\n",
    "# Добавляем столбец id с уникальным значением. Оно будет ключем для кафки. Передаём стрим напрямую в кафку. \n",
    "# Тут надо обратить внимание, что используется одна опция checkpointLocation\n",
    "# Ещё важен порядок методов - надо ставить withColumn и другие запросы после option и csv.\n",
    "raw = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"csv\") \\\n",
    "    .schema(schema) \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .csv(\"/data/db_re\") \\\n",
    "    .withColumn('id', F.concat(F.unix_timestamp(), F.rand())) \\\n",
    "    .withColumn('value', to_json(struct(schema.names))) \\\n",
    "    .selectExpr(\"id as key\", \"value as value\") \\\n",
    "    .selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "    .writeStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kf.SERVERS) \\\n",
    "    .option(\"topic\", topic_name) \\\n",
    "    .option(\"checkpointLocation\", checkpoints) \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6de8a0-25bf-42e1-8bfe-a20af2189f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf.topic_read(name='lesson3', from_beginning=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2f7625-a8c1-46b8-81a2-7f31e5b5e439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Остановим все потоки\n",
    "stop_all_streams(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7eca8c-d8a9-4d1e-839d-8d3185f2e3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удаляем топик\n",
    "kf.topic_delete(name=['lesson3'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
