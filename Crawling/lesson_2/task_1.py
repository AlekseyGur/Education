# Вариант 1
# Необходимо собрать информацию о вакансиях на вводимую должность (используем input или через аргументы получаем должность) с сайтов HH(обязательно) и/или Superjob(по желанию). Приложение должно анализировать несколько страниц сайта (также вводим через input или аргументы). Получившийся список должен содержать в себе минимум:
#
#     Наименование вакансии.
#     Предлагаемую зарплату (разносим в три поля: минимальная и максимальная и валюта. цифры преобразуем к цифрам).
#     Ссылку на саму вакансию.
#     Сайт, откуда собрана вакансия.
#
# По желанию можно добавить ещё параметры вакансии (например, работодателя и расположение). Структура должна быть одинаковая для вакансий с обоих сайтов. Общий результат можно вывести с помощью dataFrame через pandas. Сохраните в json либо csv.


from time import sleep
import pandas as pd
from task_1_tools import *

# получаем от пользователя название вакансии и нужное кол-во страниц
# для сканирования
text = input('Название вакансии: ')
num_pages = int(input('Кол-во страниц для сканирования: '))

# Список из страниц для сканирования. На текущий момент сканирование идёт по
# всем нужным страницам пагинации последовательно по каждому ресурсу. То есть
# пока все страницы первого не перебрали, то к следующему ресурсу не будет
# перехода. Стоит переделать логику, чтобы у всех ресурсов одновременно
# запрашивать сначала первую страницу, вторую и т.д. Так можно будет сэкономить
# на задержке между сканированиями страниц одного ресурса.
# В адресе обязательно должен быть GET параметр page=1 или аналогичный,
# указывающий на то, что это первая страница в постраничной навигации.
urls = [f'https://hh.ru/search/vacancy?text={text}&page=1',
        f'https://superjob.ru/vacancy/search/?keywords={text}&page=1']

vacancies = []  # тут будет список из словарей, содержащих данные о вакансиях
for url in urls:
    site = get_site_code(url)  # символьный код сайта

    # Первый раз сканируем, чтобы получить пагинацию
    dom = get_page_dom(url)  # dom BeautifulSoup с данными страницы сайта
    vacancies.extend(get_vacancies(dom, site)) # добавляем вакансии в список
    num_pages_max = get_page_max_num(dom, site) # макс. количество страниц в выборке

    # Если запрашивается больше одной страницы и есть пагинация, то повторяем
    # действие для нужного количества страниц
    if 1 < num_pages and 1 < num_pages_max:
        if num_pages_max < num_pages:  # просят больше страниц, чем есть
            num_pages = num_pages_max

        print(f'Кол-во страниц для сканирования на "{site}" = {num_pages_max}')

        for page_num in range(num_pages - 1):
            sleep(5)
            url = inc_url_page_num(url)
            dom = get_page_dom(url)
            vacancies.extend(get_vacancies(dom, site))


data = pd.DataFrame(vacancies)
print(pd.DataFrame(data))

# сохранение данных о вакансиях в файл
data.to_csv('data.csv', sep='\t')
