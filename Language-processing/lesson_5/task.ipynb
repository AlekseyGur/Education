{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bc2ed5b-a4ea-47f6-9748-0a65959a60aa",
   "metadata": {},
   "source": [
    "# Урок 5. Part-of-Speech разметка, NER, извлечение отношений"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "69df3ef7-4ae9-4c8c-9c80-efeee078a787",
   "metadata": {},
   "source": [
    "**Задание 1. Написать теггер на данных с русским языком.**\n",
    "\n",
    "1. проверить UnigramTagger, BigramTagger, TrigramTagger и их комбинации.\n",
    "2. написать свой теггер как на занятии, попробовать разные векторайзеры, добавить знание не только букв но и слов.\n",
    "3. сравнить все реализованные методы, сделать выводы.\n",
    "\n",
    "\n",
    "**Задание 2. Проверить, насколько хорошо работает NER.**\n",
    "\n",
    "Данные брать из Index of /pub/named_entities\r\n",
    "1. проверить NER из nltk/spacy/deeppavlov.\r\n",
    "2. написать свой NER, попробовать разные подходы.\r\n",
    "   1. передаём в сетку токен и его соседей.\r\n",
    "   2. передаём в сетку только токен.\r\n",
    "   3. свой вариант.\r\n",
    "3. сравнить свои реализованные подходы на качество — вывести precision/recall/f1_score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67838b2c-0b05-4018-9061-2a2f75cfb0a7",
   "metadata": {},
   "source": [
    "### Задание 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f1a0293-ee20-45cd-89f0-61fec68bf50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyconll corus razdel spacy --break-system-packages\n",
    "# !pip install -U spacy\n",
    "# !python -m spacy info\n",
    "# !python -m spacy download ru_core_news_sm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93e5bc15-389a-4f76-8aae-4b05ed426eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget -O ru_syntagrus-ud-train.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train-a.conllu\n",
    "# !wget -O ru_syntagrus-ud-dev.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-dev.conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e454559-9530-46b8-8896-415a0018fa0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-29 13:56:38.661026: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-09-29 13:56:38.661121: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-09-29 13:56:38.661150: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-09-29 13:56:38.669305: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-29 13:56:39.487836: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib\n",
    "import pyconll\n",
    "import corus\n",
    "from corus import load_ne5\n",
    "import re\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report,f1_score, accuracy_score\n",
    "from sklearn import model_selection, preprocessing, linear_model\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import GlobalMaxPooling1D, Conv1D, GRU, LSTM, Dropout, Input\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "from razdel import tokenize\n",
    "from corus import load_ne5\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import ru_core_news_sm\n",
    "from spacy.lang.ru.examples import sentences \n",
    "from spacy.lang.ru import Russian\n",
    "\n",
    "import pyconll\n",
    "import nltk\n",
    "from nltk.tag import DefaultTagger, UnigramTagger, BigramTagger, TrigramTagger, RegexpTagger\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "data_train = pyconll.load_from_file('ru_syntagrus-ud-train.conllu')\n",
    "data_test = pyconll.load_from_file('ru_syntagrus-ud-dev.conllu')\n",
    "\n",
    "fdata_train = []\n",
    "for sent in data_train[:]:\n",
    "    fdata_train.append([(token.form, token.upos) for token in sent])\n",
    "    \n",
    "fdata_test = []\n",
    "for sent in data_test[:]:\n",
    "    fdata_test.append([(token.form, token.upos) for token in sent])\n",
    "    \n",
    "fdata_sent_test = []\n",
    "for sent in data_test[:]:\n",
    "    fdata_sent_test.append([token.form for token in sent])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dd8349-2968-4d70-b9ec-f9c45b3697c7",
   "metadata": {},
   "source": [
    "### Проверить UnigramTagger, BigramTagger, TrigramTagger и их комбинации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d96ec58f-de15-4972-afb7-caa5442a132d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nltk: 0.0\n",
      "Unigram: 0.824\n",
      "Bigram: 0.60939\n",
      "Trigram: 0.178\n",
      "Bigram + Unigram: 0.82928\n",
      "Trigram + Bigram + Unigram: 0.82914\n"
     ]
    }
   ],
   "source": [
    "default_tagger = nltk.DefaultTagger('NN')\n",
    "default_acc = default_tagger.evaluate(fdata_test)\n",
    "\n",
    "unigram_tagger = UnigramTagger(fdata_train)\n",
    "unigram_acc = unigram_tagger.evaluate(fdata_test)\n",
    "\n",
    "bigram_tagger = BigramTagger(fdata_train)\n",
    "bigram_acc = bigram_tagger.evaluate(fdata_test)\n",
    "\n",
    "trigram_tagger = TrigramTagger(fdata_train)\n",
    "trigram_acc = trigram_tagger.evaluate(fdata_test)\n",
    "\n",
    "bigram_tagger = BigramTagger(fdata_train, backoff=unigram_tagger)\n",
    "bigram_unigram_acc = bigram_tagger.evaluate(fdata_test)\n",
    "\n",
    "trigram_tagger = TrigramTagger(fdata_train, backoff=bigram_tagger)\n",
    "trigram_bigram_unigram_acc = trigram_tagger.evaluate(fdata_test)\n",
    "\n",
    "print(f'nltk: {round(default_acc, 3)}')\n",
    "print(f'Unigram: {round(unigram_acc, 3)}')\n",
    "print(f'Bigram: {round(bigram_acc, 5)}')\n",
    "print(f'Trigram: {round(trigram_acc, 3)}')\n",
    "print(f'Bigram + Unigram: {round(bigram_unigram_acc, 5)}')\n",
    "print(f'Trigram + Bigram + Unigram: {round(trigram_bigram_unigram_acc, 5)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4e1c5a-8adc-4505-b55d-99f4584aade8",
   "metadata": {},
   "source": [
    "### Объединённый теггер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "940e18ce-2ba1-4ded-9cca-b54214136124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.827905462595221"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def union_taggers(train_sents, tagger_classes, backoff=None):\n",
    "    for cls in tagger_classes:\n",
    "        backoff = cls(train_sents, backoff=backoff)\n",
    "    return backoff\n",
    "\n",
    "backoff = DefaultTagger('NN') \n",
    "tag = union_taggers(fdata_train,  \n",
    "                     [UnigramTagger, BigramTagger, TrigramTagger],  \n",
    "                     backoff = backoff) \n",
    "  \n",
    "tag.evaluate(fdata_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58459c5a-87f7-4bf7-b67b-8b12d99c09f9",
   "metadata": {},
   "source": [
    "### Написать свой теггер как на занятии, попробовать разные векторайзеры, добавить знание не только букв но и слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b0b0560-a7ec-430a-8353-2580009b367c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tok = []\n",
    "train_label = []\n",
    "for sent in fdata_train[:]:\n",
    "    for tok in sent:\n",
    "        train_tok.append(tok[0])\n",
    "        train_label.append('NO_TAG' if tok[1] is None else tok[1])\n",
    "        \n",
    "test_tok = []\n",
    "test_label = []\n",
    "for sent in fdata_test[:]:\n",
    "    for tok in sent:\n",
    "        test_tok.append(' ' if tok[0] is None else tok[0])\n",
    "        test_label.append('NO_TAG' if tok[1] is None else tok[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e81003d6-10cd-4845-8304-0c52902b77bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Анкета', '.', 'Начальник', 'областного', 'управления', 'связи', 'Семен'],\n",
       " ['NOUN', 'PUNCT', 'NOUN', 'ADJ', 'NOUN', 'NOUN', 'PROPN'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tok[:7], train_label[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c4e4d43-67dc-4819-9687-a069fe571f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "train_enc_labels = le.fit_transform(train_label)\n",
    "test_enc_labels = le.transform(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f058ca97-3fa1-4341-ada1-0864a1c74791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN',\n",
       "       'NO_TAG', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM',\n",
       "       'VERB', 'X'], dtype='<U6')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea5a7f5b-6dc2-4fd8-9452-299b71b110bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vectorizer</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CountVectorizer(analyzer='char', ngram_range=(1, 5))</td>\n",
       "      <td>0.938119</td>\n",
       "      <td>0.939469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TfidfVectorizer(analyzer='char', ngram_range=(1, 5))</td>\n",
       "      <td>0.927087</td>\n",
       "      <td>0.929318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HashingVectorizer(analyzer='char', n_features=5000, ngram_range=(1, 5))</td>\n",
       "      <td>0.909308</td>\n",
       "      <td>0.912540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HashingVectorizer(analyzer='char', n_features=3000, ngram_range=(1, 5))</td>\n",
       "      <td>0.898584</td>\n",
       "      <td>0.902110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HashingVectorizer(analyzer='char', n_features=2000, ngram_range=(1, 5))</td>\n",
       "      <td>0.892840</td>\n",
       "      <td>0.896243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HashingVectorizer(analyzer='char', n_features=1000, ngram_range=(1, 5))</td>\n",
       "      <td>0.866822</td>\n",
       "      <td>0.870643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TfidfVectorizer(ngram_range=(1, 5))</td>\n",
       "      <td>0.649981</td>\n",
       "      <td>0.639508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CountVectorizer(ngram_range=(1, 5))</td>\n",
       "      <td>0.648604</td>\n",
       "      <td>0.637997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>HashingVectorizer(n_features=5000, ngram_range=(1, 5))</td>\n",
       "      <td>0.581934</td>\n",
       "      <td>0.598301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>HashingVectorizer(n_features=3000, ngram_range=(1, 5))</td>\n",
       "      <td>0.568540</td>\n",
       "      <td>0.592408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>HashingVectorizer(n_features=2000, ngram_range=(1, 5))</td>\n",
       "      <td>0.550696</td>\n",
       "      <td>0.579068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HashingVectorizer(n_features=1000, ngram_range=(1, 5))</td>\n",
       "      <td>0.341387</td>\n",
       "      <td>0.360648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                 Vectorizer  f1_score  Accuracy\n",
       "0                      CountVectorizer(analyzer='char', ngram_range=(1, 5))  0.938119  0.939469\n",
       "1                      TfidfVectorizer(analyzer='char', ngram_range=(1, 5))  0.927087  0.929318\n",
       "8   HashingVectorizer(analyzer='char', n_features=5000, ngram_range=(1, 5))  0.909308  0.912540\n",
       "7   HashingVectorizer(analyzer='char', n_features=3000, ngram_range=(1, 5))  0.898584  0.902110\n",
       "6   HashingVectorizer(analyzer='char', n_features=2000, ngram_range=(1, 5))  0.892840  0.896243\n",
       "2   HashingVectorizer(analyzer='char', n_features=1000, ngram_range=(1, 5))  0.866822  0.870643\n",
       "4                                       TfidfVectorizer(ngram_range=(1, 5))  0.649981  0.639508\n",
       "3                                       CountVectorizer(ngram_range=(1, 5))  0.648604  0.637997\n",
       "11                   HashingVectorizer(n_features=5000, ngram_range=(1, 5))  0.581934  0.598301\n",
       "10                   HashingVectorizer(n_features=3000, ngram_range=(1, 5))  0.568540  0.592408\n",
       "9                    HashingVectorizer(n_features=2000, ngram_range=(1, 5))  0.550696  0.579068\n",
       "5                    HashingVectorizer(n_features=1000, ngram_range=(1, 5))  0.341387  0.360648"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizers = [CountVectorizer(ngram_range=(1, 5), analyzer='char'), \n",
    "               TfidfVectorizer(ngram_range=(1, 5), analyzer='char'), \n",
    "               HashingVectorizer(ngram_range=(1, 5), analyzer='char', n_features=1000)] \n",
    "vectorizers_word = [CountVectorizer(ngram_range=(1, 5), analyzer='word'), \n",
    "               TfidfVectorizer(ngram_range=(1, 5), analyzer='word'), \n",
    "               HashingVectorizer(ngram_range=(1, 5), analyzer='word', n_features=1000)] \n",
    "n_features = [2000, 3000, 5000]\n",
    "vectorizers_hash = [HashingVectorizer(ngram_range=(1, 5), analyzer='char', n_features=feat) for feat in n_features]\n",
    "vectorizers_hash_word = [HashingVectorizer(ngram_range=(1, 5), analyzer='word', n_features=feat) for feat in n_features]\n",
    "f1_scores = []\n",
    "accuracy_scores = []\n",
    "\n",
    "for vectorizer in vectorizers + vectorizers_word + vectorizers_hash + vectorizers_hash_word:\n",
    "    X_train = vectorizer.fit_transform(train_tok)\n",
    "    X_test = vectorizer.transform(test_tok)\n",
    "    \n",
    "    lr = LogisticRegression(random_state=0, max_iter=100)\n",
    "    lr.fit(X_train, train_enc_labels)\n",
    "    pred = lr.predict(X_test)\n",
    "    f1 = f1_score(test_enc_labels, pred, average='weighted')\n",
    "    f1_scores.append(f1)\n",
    "    acc = accuracy_score(test_enc_labels, pred)\n",
    "    accuracy_scores.append(acc)\n",
    "    # print(vectorizer)\n",
    "    # print(classification_report(test_enc_labels, pred, target_names=le.classes_))\n",
    "\n",
    "pd.set_option('display.max_columns', None)  \n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', None)\n",
    "\n",
    "result_model = pd.DataFrame({\n",
    "    'Vectorizer': vectorizers + vectorizers_word + vectorizers_hash + vectorizers_hash_word,\n",
    "    'f1_score': f1_scores,\n",
    "    'Accuracy': accuracy_scores\n",
    "})\n",
    "result_model.sort_values('f1_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ddab794-39f1-41be-a8d8-8f99554e5fe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vectorizer</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CountVectorizer(analyzer='char', ngram_range=(1, 5))</td>\n",
       "      <td>0.938119</td>\n",
       "      <td>0.939469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TfidfVectorizer(analyzer='char', ngram_range=(1, 5))</td>\n",
       "      <td>0.927087</td>\n",
       "      <td>0.929318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HashingVectorizer(analyzer='char', n_features=5000, ngram_range=(1, 5))</td>\n",
       "      <td>0.909308</td>\n",
       "      <td>0.912540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HashingVectorizer(analyzer='char', n_features=3000, ngram_range=(1, 5))</td>\n",
       "      <td>0.898584</td>\n",
       "      <td>0.902110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HashingVectorizer(analyzer='char', n_features=2000, ngram_range=(1, 5))</td>\n",
       "      <td>0.892840</td>\n",
       "      <td>0.896243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HashingVectorizer(analyzer='char', n_features=1000, ngram_range=(1, 5))</td>\n",
       "      <td>0.866822</td>\n",
       "      <td>0.870643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TfidfVectorizer(ngram_range=(1, 5))</td>\n",
       "      <td>0.649981</td>\n",
       "      <td>0.639508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CountVectorizer(ngram_range=(1, 5))</td>\n",
       "      <td>0.648604</td>\n",
       "      <td>0.637997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>HashingVectorizer(n_features=5000, ngram_range=(1, 5))</td>\n",
       "      <td>0.581934</td>\n",
       "      <td>0.598301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>HashingVectorizer(n_features=3000, ngram_range=(1, 5))</td>\n",
       "      <td>0.568540</td>\n",
       "      <td>0.592408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>HashingVectorizer(n_features=2000, ngram_range=(1, 5))</td>\n",
       "      <td>0.550696</td>\n",
       "      <td>0.579068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HashingVectorizer(n_features=1000, ngram_range=(1, 5))</td>\n",
       "      <td>0.341387</td>\n",
       "      <td>0.360648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                 Vectorizer  f1_score  Accuracy\n",
       "0                      CountVectorizer(analyzer='char', ngram_range=(1, 5))  0.938119  0.939469\n",
       "1                      TfidfVectorizer(analyzer='char', ngram_range=(1, 5))  0.927087  0.929318\n",
       "8   HashingVectorizer(analyzer='char', n_features=5000, ngram_range=(1, 5))  0.909308  0.912540\n",
       "7   HashingVectorizer(analyzer='char', n_features=3000, ngram_range=(1, 5))  0.898584  0.902110\n",
       "6   HashingVectorizer(analyzer='char', n_features=2000, ngram_range=(1, 5))  0.892840  0.896243\n",
       "2   HashingVectorizer(analyzer='char', n_features=1000, ngram_range=(1, 5))  0.866822  0.870643\n",
       "4                                       TfidfVectorizer(ngram_range=(1, 5))  0.649981  0.639508\n",
       "3                                       CountVectorizer(ngram_range=(1, 5))  0.648604  0.637997\n",
       "11                   HashingVectorizer(n_features=5000, ngram_range=(1, 5))  0.581934  0.598301\n",
       "10                   HashingVectorizer(n_features=3000, ngram_range=(1, 5))  0.568540  0.592408\n",
       "9                    HashingVectorizer(n_features=2000, ngram_range=(1, 5))  0.550696  0.579068\n",
       "5                    HashingVectorizer(n_features=1000, ngram_range=(1, 5))  0.341387  0.360648"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_model.sort_values('Accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7533999-1ec5-470b-ac09-9c447a468cb2",
   "metadata": {},
   "source": [
    "Символьные векторайзеры все оказались лучше, чем словарные."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961799aa-d46c-42d5-95ea-3e1c5ec73592",
   "metadata": {},
   "source": [
    "### Задание 2.\n",
    "\n",
    "проверить NER из nltk/spacy/deeppavlov"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118dd861-d66f-472d-98bb-f818b12cf2dc",
   "metadata": {},
   "source": [
    "### NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "930a215b-f519-49ad-b71d-6ca448f1fd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('tagsets')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('maxent_ne_chunker')\n",
    "# nltk.download('words')\n",
    "# nltk.download('punkt')\n",
    "# nltk.help.upenn_tagset('RB')\n",
    "# nltk.help.upenn_tagset('NN')\n",
    "# nltk.help.upenn_tagset('VB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a3bc228-0702-4661-8fc1-0fa63bc1b668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Обязанности президента Польши будет исполнять Б.Коморовский Обязанности президента Польши вместо погибшего сегодня в авиакатастрофе Леха Качиньского будет исполнять маршал сейма (нижняя палата парламента) Бронислав Коморовский, передают польские СМИ. Сообщается, что именно такой механизм перехода власти предусмотрен Конституцией Польши. В течение двух недель маршал сейма обязан объявить о дате новых президентских выборов на пост главы государства. Ранее сообщалось, что МИД Польши подтвердил гибель президента страны Леха Качиньского в результате авиакатастрофы самолета под Смоленском. По словам пресс-секретаря МИД Польши Петра Пашковского, правительство Польши собирается на экстренное заседание. Сегодня в 10:50 мск в Смоленской области под г.Печерском при заходе на посадку в аэропорт Северный в условиях сильного тумана разбился самолет Ту-154 президента Польши. На борту самолета, по уточненным данным, находились 96 человек, в том числе президент Польши Л.Качиньский и его супруга, все они погибли. По данному факту возбуждено уголовное дело по ч.3 ст.263 УК (нарушение правил безопасности движения воздушного транспорта повлекшее по неосторожности смерть двух и более лиц). 10 апреля 2010г. '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records = load_ne5('Collection5/')\n",
    "document = next(records)\n",
    "text = document.text\n",
    "text = re.sub('\\r\\n\\r\\n',' ',text)\n",
    "text = re.sub('\\r\\n',' ',text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c6531845-bd5f-4185-8a1e-345da4a9c7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "document.text = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1780cb20-d19c-4893-b9fe-5b3190951d82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Обязанности', 'JJ'),\n",
       " ('президента', 'NNP'),\n",
       " ('Польши', 'NNP'),\n",
       " ('будет', 'NNP'),\n",
       " ('исполнять', 'NNP'),\n",
       " ('Б.Коморовский', 'NNP'),\n",
       " ('Обязанности', 'NNP'),\n",
       " ('президента', 'NNP'),\n",
       " ('Польши', 'NNP'),\n",
       " ('вместо', 'NNP')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(nltk.word_tokenize(text))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a212693b-1825-4156-82d1-9c3e88b2f462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('Бронислав Коморовский', 'PERSON'),\n",
       " ('МИД Польши Петра Пашковского', 'ORGANIZATION'),\n",
       " ('Польши', 'PERSON'),\n",
       " ('Ранее', 'PERSON'),\n",
       " ('Сообщается', 'PERSON')}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{(' '.join(c[0] for c in chunk), chunk.label() ) for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(text))) if hasattr(chunk, 'label') }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "efd84ce5-9e9e-4d8a-a4ef-c537b12f915d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Ne5Span(\n",
       "     index='T1',\n",
       "     type='GEOPOLIT',\n",
       "     start=23,\n",
       "     stop=29,\n",
       "     text='Польши'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T2',\n",
       "     type='PER',\n",
       "     start=46,\n",
       "     stop=59,\n",
       "     text='Б.Коморовский'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T3',\n",
       "     type='GEOPOLIT',\n",
       "     start=86,\n",
       "     stop=92,\n",
       "     text='Польши'\n",
       " )]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document.spans[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f583b39e-5681-4515-9e24-336e4eb8bc44",
   "metadata": {},
   "source": [
    "### SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0da5877e-7e01-4c0b-be6e-f8ff46f33062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Обязанности президента \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Польши\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " будет исполнять \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Б.Коморовский Обязанности\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " президента \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Польши\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " вместо погибшего сегодня в авиакатастрофе \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Леха Качиньского\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " будет исполнять маршал сейма (нижняя палата парламента) \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Бронислав Коморовский\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       ", передают польские \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    СМИ\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ". Сообщается, что именно такой механизм перехода власти предусмотрен Конституцией \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Польши\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ". В течение двух недель маршал сейма обязан объявить о дате новых президентских выборов на пост главы государства. Ранее сообщалось, что \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    МИД\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Польши\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " подтвердил гибель президента страны \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Леха Качиньского\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " в результате авиакатастрофы самолета под \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Смоленском\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ". По словам пресс-секретаря \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    МИД\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Польши\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Петра Пашковского\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       ", правительство \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Польши\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " собирается на экстренное заседание. Сегодня в 10:50 мск в \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Смоленской области\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " под г.\n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Печерском\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " при заходе на посадку в аэропорт \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Северный\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " в условиях сильного тумана разбился самолет Ту-154 президента \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Польши\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ". На борту самолета, по уточненным данным, находились 96 человек, в том числе президент \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Польши\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Л.Качиньский\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " и его супруга, все они погибли. По данному факту возбуждено уголовное дело по ч.3 ст.263 УК (нарушение правил безопасности движения воздушного транспорта повлекшее по неосторожности смерть двух и более лиц). 10 апреля 2010г. </div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nlp = spacy.load(\"ru_core_news_sm\")\n",
    "ny_bb = text\n",
    "article = nlp(ny_bb)\n",
    "displacy.render(article, jupyter=True, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f8ba016b-7ab8-4f89-805d-b59101d496a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Обязанности', 'NOUN', 'nsubj'),\n",
       " ('президента', 'NOUN', 'nmod'),\n",
       " ('Польши', 'PROPN', 'nmod'),\n",
       " ('будет', 'AUX', 'aux'),\n",
       " ('исполнять', 'VERB', 'ROOT')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ (token.text, token.pos_, token.dep_) for token in article[:5] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9936b2-267c-4984-ac47-46e2e3ffa126",
   "metadata": {},
   "source": [
    "### написать свой NER, попробовать разные подходы:\n",
    "\n",
    "- передаём в сетку токен и его соседей.\n",
    "- передаём в сетку только токен.\n",
    "- свой вариант."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cbbaef2b-e284-425a-ae1e-43f13ece4b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification_report(y_test_true, y_test_pred):\n",
    "    print(classification_report(y_test_true, y_test_pred))\n",
    "\n",
    "    print('CONFUSION MATRIX\\n')\n",
    "    print(pd.crosstab(y_test_true, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "12a73223-8579-4a21-b1c3-eae712c9f2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = load_ne5('Collection5/')\n",
    "# next(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "be1a2f8f-486f-4a0f-88a2-8c419c8fd309",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_docs = []\n",
    "for ix, rec in enumerate(records):\n",
    "    words = []\n",
    "    for token in tokenize(rec.text):\n",
    "        type_ent = 'OUT'\n",
    "        for ent in rec.spans:\n",
    "            if (token.start >= ent.start) and (token.stop <= ent.stop):\n",
    "                type_ent = ent.type\n",
    "                break\n",
    "        words.append([token.text, type_ent])\n",
    "    words_docs.extend(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "463c7538-f85d-464c-ba14-650be3751427",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_words = pd.DataFrame(words_docs, columns=['word', 'tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c9631a36-1b69-43d0-ab14-3e709578a111",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tag\n",
       "OUT         34110\n",
       "PER          3149\n",
       "ORG          2224\n",
       "LOC           777\n",
       "GEOPOLIT      730\n",
       "MEDIA         404\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_words['tag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7b3b0f26-96ff-4dee-a965-d3375c2cdfb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Обязанности</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>президента</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Польши</td>\n",
       "      <td>GEOPOLIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>будет</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>исполнять</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word       tag\n",
       "0  Обязанности       OUT\n",
       "1   президента       OUT\n",
       "2       Польши  GEOPOLIT\n",
       "3        будет       OUT\n",
       "4    исполнять       OUT"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b028df97-b0a8-4235-99b5-1b48a85c0efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(df_words['word'], df_words['tag'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4762e65f-a2b1-465c-9571-ddc5e3da10bf",
   "metadata": {},
   "source": [
    "Целевая переменная:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "afe6347c-0d53-4035-b2b5-f00fd4060c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cb06da76-3cb3-49b7-9c25-c50a95abc3ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['GEOPOLIT', 'LOC', 'MEDIA', 'ORG', 'OUT', 'PER'], dtype=object)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dc1bdd9e-fb42-4ce2-8b6e-167838f347e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "864                 г\n",
       "15055        источник\n",
       "21626               в\n",
       "20474       причинами\n",
       "15816           также\n",
       "             ...     \n",
       "1016           работы\n",
       "32474           этого\n",
       "19911               \"\n",
       "37856    оппозиционер\n",
       "39596       заседании\n",
       "Name: word, Length: 10349, dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "27708800-30fe-412d-8116-eb6ace2079d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices((train_x, train_y))\n",
    "valid_data = tf.data.Dataset.from_tensor_slices((valid_x, valid_y))\n",
    "\n",
    "train_data = train_data.batch(16)\n",
    "valid_data = valid_data.batch(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4d4c525f-cc41-4b16-af79-566b11129073",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "train_data = train_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "valid_data = valid_data.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "438211d2-921a-4c20-bd98-c2e43bb8920e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "    # Здесь может быть предобработка текста\n",
    "    return input_data\n",
    "\n",
    "vocab_size = 30000\n",
    "seq_len = 10\n",
    "\n",
    "vectorize_layer = TextVectorization(  \n",
    "                            standardize=custom_standardization,\n",
    "                            max_tokens=vocab_size,\n",
    "                            output_mode='int',\n",
    "                            # ngrams=(1, 3),\n",
    "                            output_sequence_length=seq_len)\n",
    "\n",
    "# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
    "text_data = train_data.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2ca18932-7c84-46e0-a35e-6b1b7e4a9c0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8683"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorize_layer.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d0269050-6490-43cd-bb6b-ffea8059e45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 64\n",
    "\n",
    "class modelNER(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(modelNER, self).__init__()\n",
    "        self.emb = Embedding(vocab_size, embedding_dim)\n",
    "        self.gPool = GlobalMaxPooling1D()\n",
    "        self.fc1 = Dense(300, activation='relu')\n",
    "        self.fc2 = Dense(50, activation='relu')\n",
    "        self.fc3 = Dense(6, activation='softmax') # [OUT, PER, ORG, LOC, GEOPOLIT, MEDIA]\n",
    "\n",
    "    def call(self, x):\n",
    "        x = vectorize_layer(x)\n",
    "        x = self.emb(x)\n",
    "        pool_x = self.gPool(x)\n",
    "        \n",
    "        fc_x = self.fc1(pool_x)\n",
    "        fc_x = self.fc2(fc_x)\n",
    "        \n",
    "        concat_x = tf.concat([pool_x, fc_x], axis=1)\n",
    "        prob = self.fc3(concat_x)\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1cb042d8-0712-49a0-8ecf-244b13539cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmodel = modelNER()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d58e913a-d5e4-433f-9ef9-6b044aaae8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmodel.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "97bea6f9-bc43-45a5-90f0-3b89a8bc2fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1941/1941 [==============================] - 29s 14ms/step - loss: 0.4554 - accuracy: 0.8709 - val_loss: 0.3084 - val_accuracy: 0.9153\n",
      "Epoch 2/3\n",
      "1941/1941 [==============================] - 28s 15ms/step - loss: 0.1523 - accuracy: 0.9563 - val_loss: 0.2890 - val_accuracy: 0.9299\n",
      "Epoch 3/3\n",
      "1941/1941 [==============================] - 28s 14ms/step - loss: 0.0968 - accuracy: 0.9701 - val_loss: 0.2878 - val_accuracy: 0.9309\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f624170c2d0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmodel.fit( train_data,\n",
    "            validation_data=valid_data,\n",
    "            epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "da6cd570-67ee-4bdc-a0e2-1aac84e68e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "324/324 [==============================] - 0s 948us/step\n"
     ]
    }
   ],
   "source": [
    "pred_y = mmodel.predict(valid_x)\n",
    "y_pred_classes = np.argmax(pred_y,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a3725832-0032-49ee-979e-fac1511014d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9241565722821032"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1 = f1_score(valid_y, y_pred_classes, average= \"weighted\")\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7eb3dd8d-469f-4b2f-9a83-c149a50d56bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['GEOPOLIT' 'LOC' 'MEDIA' 'ORG' 'OUT' 'PER']\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.81      0.84       195\n",
      "           1       0.92      0.68      0.78       199\n",
      "           2       0.99      0.65      0.79       107\n",
      "           3       0.91      0.52      0.66       559\n",
      "           4       0.93      1.00      0.96      8497\n",
      "           5       0.98      0.65      0.78       792\n",
      "\n",
      "    accuracy                           0.93     10349\n",
      "   macro avg       0.94      0.72      0.80     10349\n",
      "weighted avg       0.93      0.93      0.92     10349\n",
      "\n",
      "CONFUSION MATRIX\n",
      "\n",
      "col_0    0    1   2    3     4    5\n",
      "row_0                              \n",
      "0      157    2   0    5    31    0\n",
      "1        1  136   0    2    60    0\n",
      "2        0    3  70    2    29    3\n",
      "3       18    3   0  289   245    4\n",
      "4        1    4   1   20  8470    1\n",
      "5        0    0   0    0   280  512\n"
     ]
    }
   ],
   "source": [
    "print(f\"Classes: {encoder.classes_}\\r\\n\")\n",
    "\n",
    "get_classification_report(valid_y, y_pred_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59726d64-f6cc-41f0-b299-f3326a7b4f18",
   "metadata": {},
   "source": [
    "#### Обучим нейронную сеть на биграммах и триграммах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "195e3faf-4a89-4b69-bfc4-db9a7484798f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "    # Здесь может быть предобработка текста.\n",
    "    return input_data\n",
    "\n",
    "vocab_size = 30000\n",
    "seq_len = 10\n",
    "\n",
    "vectorize_layer = TextVectorization( \n",
    "                            standardize=custom_standardization,\n",
    "                            max_tokens=vocab_size,\n",
    "                            output_mode='int',\n",
    "                            ngrams=(1, 3),\n",
    "                            output_sequence_length=seq_len)\n",
    "\n",
    "# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
    "text_data = train_data.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9a7cc3ac-17ae-4534-a344-a81d12cfe8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmodel = modelNER()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3e4be935-e775-4d8f-93eb-2c7b76c5d8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmodel.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4131b888-9c89-4cf5-a8c0-3a33e788614f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1941/1941 [==============================] - 29s 15ms/step - loss: 0.4641 - accuracy: 0.8674 - val_loss: 0.3119 - val_accuracy: 0.9140\n",
      "Epoch 2/3\n",
      "1941/1941 [==============================] - 29s 15ms/step - loss: 0.1595 - accuracy: 0.9525 - val_loss: 0.2790 - val_accuracy: 0.9298\n",
      "Epoch 3/3\n",
      "1941/1941 [==============================] - 28s 14ms/step - loss: 0.0972 - accuracy: 0.9701 - val_loss: 0.2871 - val_accuracy: 0.9299\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f6199e810d0>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmodel.fit( train_data,\n",
    "            validation_data=valid_data,\n",
    "            epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0193b5f0-b74c-4402-94e1-907929383d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "324/324 [==============================] - 0s 994us/step\n"
     ]
    }
   ],
   "source": [
    "pred_y = mmodel.predict(valid_x)\n",
    "y_pred_classes = np.argmax(pred_y,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0412b11f-e841-4372-8b8b-be5aee753899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9232063420847907"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1 = f1_score(valid_y, y_pred_classes, average= \"weighted\")\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e877a3f0-f885-47a5-8a78-8ca1122e5e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['GEOPOLIT' 'LOC' 'MEDIA' 'ORG' 'OUT' 'PER']\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.76      0.82       195\n",
      "           1       0.89      0.66      0.76       199\n",
      "           2       0.99      0.77      0.86       107\n",
      "           3       0.87      0.51      0.65       559\n",
      "           4       0.93      1.00      0.96      8497\n",
      "           5       0.98      0.65      0.78       792\n",
      "\n",
      "    accuracy                           0.93     10349\n",
      "   macro avg       0.93      0.72      0.80     10349\n",
      "weighted avg       0.93      0.93      0.92     10349\n",
      "\n",
      "CONFUSION MATRIX\n",
      "\n",
      "col_0    0    1   2    3     4    5\n",
      "row_0                              \n",
      "0      149    2   0   11    30    3\n",
      "1        1  131   0    7    60    0\n",
      "2        0    3  82    2    20    0\n",
      "3       17    3   0  286   249    4\n",
      "4        1    8   1   21  8464    2\n",
      "5        0    0   0    0   280  512\n"
     ]
    }
   ],
   "source": [
    "print(f\"Classes: {encoder.classes_}\\r\\n\")\n",
    "\n",
    "get_classification_report(valid_y, y_pred_classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
